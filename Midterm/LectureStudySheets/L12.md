# Lecture 12: MapReduce
## MapReduce
#### Terms:
- **Map** - Takes an input pair and produces a set of intermediate key/value pairs
  - If map operations are independent of each other, they can be done in parallel
- **Reduce** - Accepts intermediate key and set of values for that key
  - Merges these values to get a smaller set of values
- **MapReduce Library** - Groups all intermediate values with the same intermediate key and passes them to the reduce function
- **MapReduce specification object** - contains names of input/output and tuning parameters
#### Concepts:
- Computation takes a set of input key/value pairs and produces a set of output key/value pairs

## Examples:
#### Distributed Grep
- **Map** - Emit line if it matches specified pattern
- **Reduce** - Copy intermediate data to the output (reducer is an dentity function)
#### Counts of URL access frequency
- **Map** - Process logs of web page requests
  - Output `<URL, 1>`
- **Reduce** - Add together all values for a particular URL
  - Output `<URL, total count>`
#### Reverse Web-link Graph
- **Map** - Outputs `<target, source>` pair for each `target` URL found in page `source`
- **Reduce** - Concatenate list of all sources for a `target` URL
  - Output `<target, list(source)>`
#### Term-Vector per Host
- Summarizes important terms that occur in a set of documents `<word, frequency>`
- **Map** - For each input document, emits `<hostname, term vector>`
- **Reduce** - Has all per-document vectors for a given host
  - Add term vectors, discard away infrequent terms
  - `<hostname, term vector>`
#### Inverted Index
- **Map** - Parse each document, emit `<word, document ID>`
- **Reduce** - Accept all pairs for a given `word`, sort `document IDs`, emit `<word, list(document ID)>` pair

## Implementation
#### Terms:
- **Master copy** - picks idle workers and assigns them map or reduce tasks
  - Data structures:
    - For each Map and Reduce task:
      - State (idle, in-progress, or completed)
      - Worker machine identity
    - For each completed Map task
      - Location and sizes of `R` intermediate file regions
    - Information pushed incrementally to in-progress Reduce tasks
- **Worker copies** - assigned map or reduce tasks by master
- **`M`** - number of map tasks
- **`R`** - number of reduce tasks
- **GFS** - Google File System
#### Concepts:
- Implementation: machines are commodity machines, GFS is used to manage data stored on the disks
- Execution overview:
  - Maps distributed across multiple machines
  - Automatic partitioning of data into `M` splits
  - Splits are processed concurrently on different machines
  - Partition intermediate key space into `R` pieces (eg. `hash(key) mod R`)
    - User specified parameters: Partitioning function, number of partitions (`R`)

![MapReduce Execution](https://raw.github.com/jarretflack/cs455Studying/master/Midterm/images/L12-map-reduce-execution.png?raw=true)
#### Execution:
- **Step 1: The MapReduce library**
  - Splits input files into `M` pieces (16 - 64 MB per piece)
  - Starts up copies of the program on a cluster of machines
- **Step 2: Program copies**
  - One copy is a master: picks idle workers and assigns them map or reduce tasks
  - There are `M` map tasks and `R` reduce tasks
- **Step 3: Workers that are assigned a map task**
  - Parse `<key, value>` pairs out of input data
  - Pass each pair to user-defined Map function
  - Get intermediate `<key, value>` pairs from Maps
- **Step 4: Writing to disk**
  - Buffered pairs are periodically written to disk
  - Writes are partitioned by the partitioning function
  - Locations of buffered pairs are reporterd to master so they can be forwarded to Reduce workers
- **Step 5: Reading intermediate data**
  - Master notifies Reduce worker about locations
  - Reduce worker reads buffered data from the local disks of Maps
  - Read all intermediate data and sort by intermediate key (occurences of the same key are grouped)
- **Step 6: Processing data at the Reduce worker**
  - Iterate over sorted data and pass `<key, intermediate value>` to Reduce function
  - Output is appended to output file of the reduce partition
- **Step 7: Waking up the user**
  - After all map/reduce tasks have been completed, control returns to the user code

## Fault Tolerance
#### Terms:
- **Worker failures** - Master periodically pings worker; after a number of failed pings the master marks worker as having failed.
- **Master failures** - MapReduce computation is aborted: client must check and retry MapReduce operation
- **Locality** - conserve network bandwidth: input files managed by GFS
  - MapReduce master takes location of input files into account and schedules task on a machine that contains a replica of the input slice
  - Therefore, most input data is read locally and consumes no network bandwidth
#### Concepts:
- Any Map task completed by a failed worker is reset to its initial idle state and is eligible for rescheduling
  - This is because the output is stored on local disk of failed machine; therefore it is inaccessible
  - All workers are notified about reexecution
- Reduce tasks do not need to be reexecuted since the output is stored in the GFS
- If map and reduce operations are deterministic, distributed execution output is identical to non-faulting, sequential execution.
  - Atomic commits of map and reduce task outputs help acheive this
- Each in-progress task writes output to private temporary files
  - Map task produces `R` such files, sent to master after task completed
  - Reduce task produces one file, when task completes worker atomically renames temp file to final output file (using GFS)
