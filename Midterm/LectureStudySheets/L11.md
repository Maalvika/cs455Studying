Synchronizers:

  * Semaphores
      * Counting semaphores control the number of activities that can (1) Access a certain resources and (2) perform a given action
      * They are used to implement resource pools or impose bounds on a collection
      * Semaphores are useful for implementing resource pools
      * These resouce pools should be blocking to account for zero or more resources
      * Initialize the semaphore to the pool size
      * Syncrhonization through permits: Manage a set of virtual permits (number passed to the constructor) -- acquire and release permits should be blocking as well and they either grab or return a permit to the semaphore
      * Before fetching a resource from the pool you must acquire a permit
      * Release the permit after putting the resource back in the pool
      * acquire blocks until the pool is non-empty
      
  * Binary Semaphores
      * Semaphore with an initial count of 1
      * can be used as a mutex with non-reentrant locking semantics.  Whoever holds the permit holds the mutex
      
  * Barriers
      * Barriers are similiar to latches in that they block a group of threads till an event has occurred
      * All threads must come together at barrier point at the same time to proceed
      * Latches wait for events, Barriers wiat for other threads
      * Barriers are often used in simulations where work to calculate one step can  be done in parallel.  They barrier allows all work associated with a given step to be completed before advancing to the next step
      * I.E All threads complete step K, before moving on to step K+1
      
  * Cyclic Barrier
      * Allows a fixed number of parties to rendezvous at a fixed point
      * Useful in parallel iterative algorithms where problem can be broken into a fixed number of independent subproblems
      
  * Exchanger
      * This is another type of barrier (two-party)
      * Parties exchange data at the barrier point
      * Useful when asymmetric activities are performed (I.E Producer-Consumer problem)
      * 2 threads exchange objects via Exchanger
      * Safe publication of objects to other party
      
Thread Safety Summary:

  * The less mutable state there is, the easier it is to ensure thread safety
  * Make fields final unless they need to be mutable
  * Immutable objects are always thread safe
  * Encapsulation makes it practical to manage complexity
  * Guard each mutable variable with a lock
  * Guard all varibales with an invariant with the same lock(property that holds for all instances of a class, always, no matter what other code does)
  * Use a set of standard synchronization primitives to control access to shared state
  
Cloud Computing

  * Data transefers can be improved by using multiple disks
  
  * There's more than just reading and writing from multiple disks in parallel 
      * cope with hardware failures 
      * combine datasets that are dispered over mutliple disks)
      
  * MapReduces provides:
      * Programming model that abstracts the problem from disk reads and writes
      * Transfrom the problem into computation over sets of keys and values
      * Supports distributed processing on large datasets over a cluster of computers
      
  * Why not use databases with lots of disks?
    * Another trend in disk drives:
        * Seek time is improving much slower than transfer rates
    * If data access pattern is dominated by seeks?
    * It takes longer to read or write large portions of the dataset than streaming through it
        * Streaming through dataset operates at transfer speed    
        
  * MapReduce should be seen as complementary to databases
     * MapReduce is good for problems that access the entire dataset (I.E ad hoc analysis, write once, read many times)
     * RDBMS is good for point queries or updates
        * Dataset has been indexed for low-latency retiries and update times
        * read and writes many times
  
  * Grid Computing/HPCS systems
     * Distribute work across a cluster of machines that access a shared file system
     * Works well for predominantly computation-intensive jobs
          * problems when access to large data volume is needed
              * Network bandwidth is a bottleneck and compute nodes become idle
              
  * MapReduce tries to keep data with the compute node side by side
    * Data Locality: 
        * Data access is fast since it is local and conserves network bandwidth
        
 MapReduce
 
  * Complexity of managing distributed computations can:
      * (1) Obscure simplicity of original computation
      * (2) Contributing factors:
            * How to parallelize the computation
            * Distribute the data
            * Handle failures
            
  * MapReduce handles this complexity
      * Hides messy details of parallelization, data distribution, fault tolerance, and load balancing
    
  * MapReduce is a programming model  (associated implementation for processing & generating large data sets)
      * computation takes a set of key/value pairs and outputes a set of key/value pairs
      * Express computation as two functions:
          * Map-Takes an input pair and produces a set of intermediate key/value pairs
               -Mapper operation are independent of each other if they can be performed in parallel (i.e they share nothing)
               -the MapReduce library groups all intermediate values with the same intermediate key
               -The group of intermediate values and key are passed to the Reduce function
          * Reduce-Accepts intermediate key I and set of values for taht key. Merge these values together to get smaller set of value
          
  
  
  
