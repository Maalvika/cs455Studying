# Lecture 13: MapReduce and Hadoop
## MapReduce Refinements
#### Terms:
- **Partitioning function** - Partitions data; default: `hash(key) mod R`
  - Users specify `R`, the number of output files
  - If output keys are URLs, entries from a host must go to same output file: `hash(Hostname(urlkey)) mod R`
- **Combiner function** - Does partial merging of the data before it is sent to reducer
  - Executes on each machine performing a map task
  - Code implementing combiner & reduce function is usually the same
  - Output written to intermediate file
- **Signal handler** - Each worker has a signal handler to catch segmentation violations and bus errors
  - When running into an error, signal handler sends last gasp UDP packer to Master. If Master recognizes more than one failure, the record is skipped during the next iteration.
#### Concepts:
- Intermediate key/pairs are processed in increasing order: easier to generate sorted output file
- **Combine:** Output written to intermediate file
- **Reduce:** Output written to final output file
- Text mode treats every line as a `<key, value>` pair
  - Key: offset in the file
  - Value: contents of the line
  - `<key, value>` pairs are sorted by key
  - Text splitting occurs only at line boundaries
- Besides intermediate files, other auxiliary files may be produced (side-effects)
  - No atomic commits for multiple auxiliary files that are produced
- Skipping Bad Records
  - Bugs in user code may cause Map or Reduce functions to crash; however, it is not always feasible to fix them.
  - Optional mode of operation: detect records that cause deterministic crashes and skip them.
  - If a machine fails more than once, it is skipped for the next iteration.
- Local Execution
  - Support for sequential execution of MapReduce operations on a single machine.
  - Controls to limit computation to a particular map
  - Invoke programs with a special flag: use debugging and nesting tools
  
## Hadoop
#### Terms:
- **Hadoop** - Java-based open-source implementation of MapReduce, created by Doug Cutting
- **HDFS** - Hadoop Distributed File System, based on Google File System
#### Concepts:
- Hadoop uses its own data types
  - LongWritable, Text, IntWritable, ...
- Hadoop Active Releases
  - 2.6.x
  - 2.7.x
  - 2.8.x: branched off from 2.7.0
  - 2.9.x: branched off from 2.8.2
  - 3.0.0: branched off from 2.7.0
- Features implemented in 2.3:
  - New MapReduce runtime (MapReduce 2), implemented on YARN (Yet Another Resource Negotiator)
  - HDFS federation - HDFS namespace can be dispersed across multiple name nodes
  - HDFS high-availability - removes name node as a single point of failure, supports standby nodes for failover
- Features implemented in 3.0.0
  - Support for erasure encoding in HDFS
  - Support for more than 2 name nodes in standby mode
  - Support for opportunistic containers and distributed scheduling
  - Support for Microsoft Azure Data Lake and Aliyun Object Storage System file system connectors
- Latest Releases
  - Dec. 14, 2017 - v2.7.5, Dec. 12, 2017 - v.2.8.3
  - Users are advised to wait for future releases to use v2.9.0 and v3.0.0
- Hadoop Ecosystem:

  ![Hadoop Ecosystem](https://raw.github.com/jarretflack/cs455Studying/master/Midterm/images/L13-hadoop-ecosystem.png?raw=true)

## MapReduce Jobs
#### Terms:
- **MapReduce job** - One unit of work, consisting of:
  - Input Data
  - MapReduce Program
  - Configuration information
- **MapReduce task** - Subdivision of a job: map tasks and reduce tasks
- `Mapper` - 4 parameters: input key/value, output key/value
  - Declares an abstract `map()` method
- `Reducer` - 4 parameters: input key/value, output key/value
  - Declares an abstract `reduce()` method
#### Concepts:
- Controlling the job execution process:
  - Older Versions
    - Job tracker - Coordinates all jobs by scheduling tasks to run on task trackers, records overall progress of each job
    - Task tracker - Run tasks and report progress to job tracker
  - Newer Versions
    - Resource Manager
    - Application Manager
    - Node Manager
- Processing a dataset
  - Step 1: Divide work and execute concurrently on multiple machines
  - Step 2: Combine results from independent processes
  - Step 3: Deal with failures that might take place
- Example: Processing a weather dataset
  - Each line is a record
  - Map:
    - Each line is a **value**
    - **Key** is the offset of the beginning of the line from the beginning of the file
    - Function: Pulls out year and air temperature
      - Sort values by key and order in intermediate step
  - Reduce:
    - Iterate through list of temperatures and find the maximum
- Code must be packaged in a JAR file for Hadoop to distribute over the cluster
- `waitforCompletion()` - submits job and waits for it to complete, returns true/false
    
